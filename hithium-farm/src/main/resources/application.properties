#mongo
#mongodb 连接ip:port,  实例：127.x.x.1:xxxx,127.x.x.2:xxxx,127.x.x.3:xxxx
mongo.host=192.168.241.170:27017
mongo.port=27017
mongo.database=ess
mongo.username=ess
mongo.password=ess
#mqtt
mqtt.server=tcp://42.192.214.33:11885
#mqtt.server=tcp://192.168.241.151:1883
#mqtt.server=tcp://192.168.241.148:1884
#mqtt.server=tcp://192.168.241.134:1883
#mqtt.server = ssl://42.192.214.33:8883
mqtt.username=admin
mqtt.password=public
mqtt.clientid.subscriber=d4f6adccb1cd441fb33b9097c2d1e921-1
mqtt.clientid.publisher=d4f6adccb1cd441fb33b9097c2d1e921-2
#共享订阅模式   $local 只在本节点创建订阅与路由表，不会在集群节点间
# $local/$share/g1/
#connected : $local/$share/g1/      disconnected $local/$share/g1/
#$queue/$ESS/+/MSG/D2S,$queue/$SYS/brokers/
#$queue/ 开头的 topic 例如 $queue/topic1，如果有多个客户端同时订阅了，EMQ 会把发送到 topic1 的消息以负载均衡的方式派发给所有客户端
#mqtt.subtopic=$queue/$ESS/+/MSG/D2S,$queue/$SYS/brokers/+/clients/+/connected,$queue/$SYS/brokers/+/clients/+/disconnected
#mqtt.subtopic=$share/g1/$ESS/+/MSG/D2S,$share/g1/$SYS/brokers/+/clients/+/connected,$share/g1/$SYS/brokers/+/clients/+/disconnected
mqtt.subtopic=$ESS/+/MSG/D2S,$SYS/brokers/+/clients/+/connected,$SYS/brokers/+/clients/+/disconnected
# 本地 共享 订阅
#mqtt.subtopic=$local/$share/g1/$ESS/+/MSG/D2S,$local/$share/g1/$SYS/brokers/+/clients/+/connected,$local/$share/g1/$SYS/brokers/+/clients/+/disconnected
mqtt.inflight=200000
mqtt.keepalive.interval=900
# 动作完成超时 单位毫秒
mqtt.complete.timeout=100000
#kafka
# 指定kafka 代理地址，可以多个
#spring.kafka.bootstrap-servers=192.168.241.170:9092
#kafka发送的重试次数
spring.kafka.producer.retries=2
#kafka重试之间的间隔 默认100ms  估算一下可能的异常恢复时间，这样可以设定总的重试时间大于这个异常恢复时间，以此来避免生产者过早地放弃重试
spring.kafka.producer.retries.backoff.ms=1000
#这个配置保证了，follwer同步完成后，才认为消息发送成功。
#spring.kafka.producer.acks=all
# 每次批量发送消息的数量 设备上报一条 400字节 设置为100000  250台设备 2500台设备同时上报16384
spring.kafka.producer.batch-size=100000
#默认为0  无延迟  linger.ms决定了你的消息一旦写入一个Batch，最多等待这么多时间,到时间后会将数据一起发送出去
#测试10S  10S后发送到kakfa        设置延迟0.1S
spring.kafka.producer.properties.linger.ms=100
spring.kafka.producer.buffer-memory=33554432
#扩大两倍
#spring.kafka.producer.buffer-memory=67108864
kafka.send.topic=t_1
#max.request.size  
spring.kafka.producer.properties.max.request.size=10485760
#request.timeout.ms = 30000
spring.kafka.producer.properties.request.timeout.ms=60000

#redis 配置
#访问地址
redis.host=192.168.241.170
##访问端口
redis.port=6379
#注意，如果没有password，此处不设置值，但这一项要保留
redis.password=redis.hithium.pwd
#最大空闲数，数据库连接的最大空闲时间。超过空闲时间，数据库连接将被标记为不可用，然后被释放。设为0表示无限制。
redis.maxIdle=100
#最小空闲数
redis.minIdle=50
#连接池的最大数据库连接数。设为0表示无限制
redis.maxActive=100
#最大建立连接等待时间。如果超过此时间将接到异常。设为-1表示无限制。
redis.maxWait=100000
#在borrow一个jedis实例时，是否提前进行alidate操作；如果为true，则得到的jedis实例均是可用的；
redis.testOnBorrow=false
#链接超时时间
redis.timeout=100000
#redis.timeout=1000
#每天晚上12点半执行一次  执行告警日志未修改的状态修改回来
#untrans.report.interval=0 30 0 * * ?
untrans.report.interval=0 30 0 * * ?
#修改告警日志状态开始时间  距离现在时间多少天  生产上为2天
alter.breakdownlog.status.start=30
##修改告警日志状态结束时间  距离现在时间多少天  生产上为1天
alter.breakdownlog.status.end=1
#3秒钟发送kafka一次
energy.untrans.report.interval=0/1 * * * * ?
#是否开启redis数据缓存
redis.data.cache.switch=true
# 是否发送kafka数据
send.to.kafka=false
###########################################kafka about config #######################################################
spring.kafka.bootstrap-servers=192.168.241.170:9092,192.168.241.168:9092,192.168.241.167:9092
##########################producer about config##############################
#spring.kafka.producer.acks=1
#spring.kafka.producer.batch-size=16384
#spring.kafka.producer.retries=0
#spring.kafka.producer.buffer-memory=33554432
#spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
#spring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer
#########################consumer about config##############################
#消费者手动提交消费数据信息
#spring.kafka.consumer.enable-auto-commit=false
#spring.kafka.listener.ack-mode=manual
#spring.kafka.consumer.group-id=test-consumer
#spring.kafka.consumer.auto-commit-interval=100
#spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
#spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer
#批量消费者 配置
# 一次poll最多返回的记录数 不代表一定到这个值才返回 有可能返回一条
#spring.kafka.consumer.max-poll-records=10
#设置并发量，小于或等于Topic的分区数  超过分区数会导致线程空闲
#spring.kafka.listener.concurrency=

